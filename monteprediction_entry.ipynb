{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microprediction/monteprediction_colab_examples/blob/main/monteprediction_entry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sector Monte Carlo Game\n",
        "\n",
        "A game where you submit one million Monte Carlo samples of sector ETF weekly returns and are rewarded based on how many of your samples are close to the ground truth (compared, that is, to how many of your competitors are also close - precise details below). Challenge yourself against other participants wielding what they claim to be SOTA methods for modeling joint distributions. You can enter merely by modifying this example notebook and running it. You can choose to follow the same rough pattern, or devise an entirely different method of generating representative samples of returns of the eleven sector ETFs measured from close to close over a one week period.  \n",
        "\n"
      ],
      "metadata": {
        "id": "8wu0RqylugX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install scipy\n",
        "!pip install --upgrade monteprediction\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from monteprediction import SPDR_ETFS\n",
        "from monteprediction.calendarutil import get_last_wednesday\n",
        "from monteprediction.submission import send_in_chunks\n",
        "\n",
        "# Factory defaults\n",
        "num_samples_per_chunk = int(1048576/8)\n",
        "num_chunks = 8\n",
        "num_samples = num_chunks*num_samples_per_chunk"
      ],
      "metadata": {
        "id": "mx0ZUY32_CAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Create a dataframe with just over one million hypothetical weekly returns for each sector.   \n",
        "\n",
        "Do this however you like this is just an example. One column per sector."
      ],
      "metadata": {
        "id": "PWvz4C7vBVVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This example uses Quasi-Monte Carlo on the empirical covariance\n",
        "# There is absolutely no requirement you follow this pattern\n",
        "\n",
        "from scipy.stats.qmc import MultivariateNormalQMC\n",
        "from sklearn.covariance import EmpiricalCovariance\n",
        "\n",
        "# Get historical weekly returns\n",
        "last_wednesday = get_last_wednesday()\n",
        "num_weeks = int(52+4*52*np.random.rand())\n",
        "start_date = last_wednesday - timedelta(weeks=num_weeks)\n",
        "data = yf.download(SPDR_ETFS, start=start_date, end=last_wednesday, interval=\"1wk\")\n",
        "weekly_prices = data['Adj Close']\n",
        "weekly_returns = weekly_prices.pct_change().dropna()\n",
        "\n",
        "# Use cov estimation to generate samples\n",
        "cov_matrix = EmpiricalCovariance().fit(weekly_returns).covariance_\n",
        "qmc_engine = MultivariateNormalQMC(mean=np.zeros(len(SPDR_ETFS)), cov=cov_matrix)\n",
        "samples = qmc_engine.random(num_samples)\n",
        "df = pd.DataFrame(columns=SPDR_ETFS, data = samples)\n",
        "print(df[:3])\n",
        "\n",
        "# Verify submission\n",
        "assert len(df.index)==num_samples,f'Expecting exactly {num_samples} samples'\n",
        "assert list(df.columns)==SPDR_ETFS,'Columns should match SPDR_ETFS in order'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZtSzrXdBL__",
        "outputId": "2848ba53-234a-4211-e7e3-590c4a0298b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  11 of 11 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        XLB       XLC       XLE       XLF       XLI       XLK       XLP  \\\n",
            "0  0.028453  0.004129  0.066391  0.032033  0.015521 -0.002501  0.011955   \n",
            "1 -0.020548 -0.012266 -0.063999 -0.054679 -0.027622  0.005228 -0.014539   \n",
            "2 -0.007326 -0.016351 -0.000677  0.014795  0.015111  0.003505  0.032404   \n",
            "\n",
            "       XLRE       XLU       XLV       XLY  \n",
            "0  0.038596  0.006206  0.008662  0.020514  \n",
            "1 -0.017256 -0.016046 -0.010144 -0.026960  \n",
            "2 -0.004032  0.034872  0.023066 -0.010453  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Submit the dataframe"
      ],
      "metadata": {
        "id": "UlqcDSD5DXNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_EMAIL = 'monteprediction_entry@monteprediction.com'  # Be sure to change this\n",
        "send_in_chunks(df, num_chunks=num_chunks, email=YOUR_EMAIL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WwTq_rwDVB1",
        "outputId": "f8e78d2b-d8a0-428c-92e8-4acb95f80d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 0 of 8 sent successfully.\n",
            "Chunk 1 of 8 sent successfully.\n",
            "Chunk 2 of 8 sent successfully.\n",
            "Chunk 3 of 8 sent successfully.\n",
            "Chunk 4 of 8 sent successfully.\n",
            "Chunk 5 of 8 sent successfully.\n",
            "Chunk 6 of 8 sent successfully.\n",
            "Chunk 7 of 8 sent successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining the game ...\n",
        "Here's how the reward system works assuming you are participant $i$. Your samples $\\{x_{ik}\\}_{k=0}^{n-1}$, where each $x_{ik}$ for $k=0,\\dots,2^{20}-1$ is an 11-vector, are used to imply an unnormalized prediction density for $z \\in \\mathbf{R}^{11}$ as:\n",
        "\n",
        "$$\\rho_i(z) = \\frac{1}{n} \\sum_{k=0}^{n-1} \\exp(-a \\|x_{ik}-z \\|_2) $$\n",
        "\n",
        "where $a$ is a system parameter set at approximately $a=300$.\n",
        "\n",
        "Let us suppose you have an initial wealth $W_i$. A system parameter $b_i=0.1$ is the fraction of your total wealth you deploy. You are considered to invest $\\Omega_i = b_i W_i$ and similarly for other participants yielding a total investment of $\\Omega = \\sum_i \\Omega_i$. This pot will be split when the truth $z$ is revealed.\n",
        "\n",
        "To this end your 'mass' is $Q_i(z) = \\Omega_i \\rho_i(z)$ represents loosely how many of your samples are close to $z$ weighted by your wealth, and the total mass near $z$ supplied by all participants is $Q(z) = \\sum_i Q_i(z)$. Your payout is your propotional share of total investment, namely $\\Omega \\frac{Q_i(z)}{Q(z)}$. Your net profit is $\\delta_i(z) = \\Omega \\frac{Q_i(z)}{Q(z)} - \\Omega_i$.  *italicized text*\n",
        "\n",
        "It should be apparent that $Q$ plays the role of an unnormalized market probability (i.e. risk-neutral density) and further, that a participant with perfect knowledge of the true density $P$ will at worst break even against any opponents' play, subject only to the ability to approximate $P$ with a collection of Monte Carlo paths in this fashion.\n",
        "\n",
        "Because this entry has been included in the mix, and is not particularly clever, there is a subsidy for participation for anyone taking even a moment to reflect statistically upon the problem (for instance, by applying shrinkage to the covariance estimation or fixing the 1-margins).\n"
      ],
      "metadata": {
        "id": "S38mSrPNDbsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'score\" is your density\n",
        "#    distances = np.linalg.norm(samples - z, axis=1)\n",
        "#    score = np.sum(np.exp(-h * distances))\n",
        "\n",
        "from monteprediction.truth import get_most_recent_truth\n",
        "from monteprediction.scoring import compute_score\n",
        "z = get_most_recent_truth()\n",
        "score = compute_score(samples=df.values,z=z)\n",
        "print(f\"Total Score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNFHQsO4FW8j",
        "outputId": "ffeaa83d-cb67-40c9-b62b-45133d3d06cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  11 of 11 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Score: 0.06990740503342224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some suggestions\n",
        "\n",
        "Ask GPT!\n",
        "\n",
        "*   It will send you to LedoitWolf or ShrunkCovariance from [sklearn.covariance](https://scikit-learn.org/stable/modules/covariance.html)\n",
        "*   Feel free to use covariance estimation methods from [precise](https://github.com/microprediction/precise/tree/main/precise/skaters/covariance)\n",
        "\n",
        "\n",
        "Copula approach:\n",
        "\n",
        "\n",
        "* For each financial return series in weekly_returns, fit a marginal distribution. This could be any distribution that fits your data well (e.g., normal, t-distribution, etc.). Use techniques like Maximum Likelihood Estimation (MLE) or Kernel Density Estimation (KDE) for this fitting.\n",
        "Transform to Uniform Marginals:\n",
        "\n",
        "\n",
        "* Apply the cumulative distribution function (CDF) of the fitted marginal distributions to transform your data into uniform marginals.\n",
        "\n",
        "* Choose an appropriate copula to model the dependence structure. Common choices include Gaussian, t, Clayton, Gumbel, and Frank copulas. The choice depends on the nature of dependence you're modeling (e.g., tail dependence).\n",
        "You may need to test different copulas to see which one fits your data best.\n",
        "\n",
        "* Fit the selected copula to the uniform marginals. This usually involves estimating parameters that best capture the dependence structure in your data.\n",
        "Techniques like Maximum Likelihood or Inversion of Kendall's Tau can be used for parameter estimation.\n",
        "\n",
        "* Once the copula is fitted, use it to generate new samples of uniform marginals that maintain the dependence structure of your original data.\n",
        "Transform these uniform marginals back to the original scale using the inverse CDFs of the marginal distributions you fitted initially. These transformed values are your simulated data points.\n",
        "Reproducibility and Randomness:\n",
        "\n",
        "Python libraries like scipy, statsmodels, or copulas can be useful.\n"
      ],
      "metadata": {
        "id": "kRDDZo6iyfMW"
      }
    }
  ]
}